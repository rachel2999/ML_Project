{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7xVxqUwuJYTdd4jiTSfMA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","\n","class EarlyStopper:\n","    def __init__(self, metric_name, patience=1, min_delta=0, minimize=True):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_val_metric = None\n","        self.max_val_metric = None\n","        self.name = metric_name\n","        self.minimize = minimize\n","\n","    def min_criteria(self, val_metric, model):\n","\n","      print(val_metric, self.min_val_metric)\n","\n","      if self.min_val_metric is None or val_metric < self.min_val_metric:\n","          self.min_val_metric = val_metric\n","          self.counter = 0\n","          torch.save(model.state_dict(), \"best_model.pth\")\n","      elif val_metric > (self.min_val_metric + self.min_delta):\n","          self.counter += 1\n","          if self.counter >= self.patience:\n","              return True\n","      return False\n","\n","    def max_criteria(self, val_metric, model):\n","      if self.max_val_metric is None or val_metric > self.max_val_metric:\n","          self.max_val_metric = val_metric\n","          self.counter = 0\n","          torch.save(model.state_dict(), \"best_model.pth\")\n","      elif val_metric < (self.max_val_metric - self.min_delta):\n","          self.counter += 1\n","          if self.counter >= self.patience:\n","              return True\n","      return False\n","\n","\n","    def early_stop(self, val_metrics, model):\n","      val_metric = val_metrics['val_' + self.name]\n","      if self.minimize:\n","        return self.min_criteria(val_metric, model)\n","      else:\n","        return self.max_criteria(val_metric, model)\n","\n","def train_one_epoch(model, loss, optimizer, ds, l2_regularization=None):\n","\n","  ''' model - pytorch model\n","  loss -  pytorch loss\n","  optimizer - pytorch optimizer\n","  ds  -  pytorch dataset'''\n","\n","  model.train() # we switch the model to the training mode\n","  train_loss = 0 # this variable accumulates loss\n","  ds_len = 0 # len of the dataset\n","  # loop over batches of the training set \n","  for x, y in ds:\n","    # print(\"HERE\", x.shape, y.shape)\n","    x, y = x.cuda(), y.cuda()\n","    optimizer.zero_grad()\n","\n","    output = model(x) # forward pass of the model \n","    \n","    # we calculate loss and gradients for optimization\n","    l = loss(output, y)\n","    if l2_regularization:\n","      l+= l2_regularization.calculate(model)\n","\n","    l.backward()\n","\n","    # optimizer updates weights of the model \n","    optimizer.step()\n","\n","    # loss record \n","    train_loss += l.item()*x.shape[0]\n","    ds_len += x.shape[0]\n","\n","  return train_loss/ds_len\n","\n","\n","def test(model, ds):\n","  ''' model - pytorch model\n","  ds  -  pytorch dataset'''\n","\n","  model.eval() # we switch the model to the evaluation mode\n","  final_output  = []\n","  # loop over batches of the test set\n","  for x, y in ds:\n","    x, y = x.cuda(), y.cuda()\n","    # we say that we do not want to calculate gradient for optimization\n","    with torch.no_grad():\n","      output = model(x) # forward pass of the model \n","      # we collect all outputs of model\n","      final_output.append(output.detach().cpu())\n","\n","  return torch.cat(final_output)\n","\n","\n","\n","def validate(model, val_metrics, ds):\n","  ''' model - pytorch model\n","  val_metrics -  dictionary of metrics\n","  ds  -  pytorch dataset'''\n","\n","  y_test = torch.cat([y for x, y in ds])\n","\n","  model_pred = test(model, ds)\n","  metric_out = {}\n","\n","  for name, metric in val_metrics.items():\n","    metric_out['val_' + name] = metric(model_pred, y_test)\n","  \n","  return metric_out\n","\n","\n","def train(model, loss, val_metrics, optimizer, train_ds, dev_ds, num_epochs=10,\n","          early_stopper=None, l2_regularization=None):\n","\n","  ''' model - pytorch model\n","  loss -  pytorch loss\n","  optimizer - pytorch optimizer\n","  train_ds  -  pytorch dataset for training\n","  dev_ds  -  pytorch dataset for evaluation while training\n","  num_epochs - number of epochs,that defines the number times that the learning \n","                algorithm will work through the entire training dataset\n","  '''\n","  # here we record parameters of network after each epoch\n","  # param_history = []\n","\n","  history = {\"train_loss\": []}\n","\n","  for key in val_metrics:\n","    history['val_' + key] = []\n","\n","\n","  for epoch in range(num_epochs):\n","\n","    # if epoch == 0:\n","    #   param_history.append(get_weights(model))\n","      # print(param_history[-1])\n","\n","    print('=========')\n","    current_train_loss = train_one_epoch(model=model, loss=loss, \n","                                         optimizer=optimizer, ds=train_ds, \n","                                         l2_regularization=l2_regularization)\n","\n","    val_metric_out = validate(model=model, val_metrics=val_metrics, ds=dev_ds)\n","\n","    for name, vm in val_metric_out.items():\n","      history[name].append(vm)\n","\n","\n","    history[\"train_loss\"].append(current_train_loss)\n","\n","    output2print = \"epoch {}\".format(epoch + 1) + \\\n","                    \" train loss: {:.4f} \".format(current_train_loss) + \\\n","                    \" \".join(\"{}: {:.4f}\".format(k, v) for k, v in val_metric_out.items())\n","\n","    print(output2print)\n","\n","\n","    if early_stopper is not None and early_stopper.early_stop(val_metric_out, model):\n","      print(\"EARLY STOPPING \")\n","      model.load_state_dict(torch.load(\"best_model.pth\"))\n","      return history\n","                      \n","  return history\n","\n","\n","class L2Regularization:\n","    def __init__(self, l2_lambda=0.01):\n","        self.l2_lambda = l2_lambda\n","\n","\n","    def calculate(self, model):\n","      l2_reg = torch.tensor(0.,  device='cuda:0')\n","      for param in model.parameters():\n","          l2_reg += torch.norm(param) \n","      #     print(\"HERE2\",l2_reg)\n","        \n","      # print(\"HERE\",l2_reg)\n","\n","      return l2_reg*self.l2_lambda\n","\n"],"metadata":{"id":"tA-E9bsTNhN-","executionInfo":{"status":"ok","timestamp":1670841633343,"user_tz":-60,"elapsed":188,"user":{"displayName":"Nicolae Banari","userId":"02726718402918962079"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iI1QHbKR1Y4t"},"execution_count":null,"outputs":[]}]}